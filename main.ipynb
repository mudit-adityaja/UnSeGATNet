{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJD5jxsBUhfj",
        "outputId": "af733a42-77cf-43d0-f9db-7fea5c203203"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.4.1+cu121\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtMZZwytUUL1"
      },
      "outputs": [],
      "source": [
        "# !wget https://isic-challenge-data.s3.amazonaws.com/2018/ISIC2018_Task1-2_Test_Input.zip\n",
        "# !wget https://isic-challenge-data.s3.amazonaws.com/2018/ISIC2018_Task1_Test_GroundTruth.zip\n",
        "# !unzip ISIC2018_Task1-2_Test_Input.zip.1\n",
        "# !unzip ISIC2018_Task1_Test_GroundTruth.zip\n",
        "\n",
        "# !rm ISIC2018_Task1-2_Test_Input.zip.1\n",
        "# !rm ISIC2018_Task1_Test_GroundTruth.zip\n",
        "folder1 = \"./CVC-ColonDB/images\"\n",
        "folder2 = \"./CVC-ColonDB/masks\"\n",
        "# folder1 = \"./ISIC2018_Task1-2_Test_Input\"\n",
        "# folder2 = \"./ISIC2018_Task1_Test_GroundTruth\"\n",
        "\n",
        "images = sorted(os.listdir(folder1))\n",
        "masks = sorted(os.listdir(folder2))\n",
        "\n",
        "images = [folder1 + '/' + x for x in images]\n",
        "masks = [folder2 + '/' + x for x in masks]\n",
        "\n",
        "images = images[1:-1]\n",
        "masks = masks[1:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LCGsGhfUaN1",
        "outputId": "08eb6bfc-256a-4302-82fc-2059e5e6e4ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(378, 378)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(masks), len(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Gv2wjg9lUe_M",
        "outputId": "c4f03e8d-1498-44e0-9f19-3e92de6bd012"
      },
      "outputs": [],
      "source": [
        "for a,b in zip(masks,images):\n",
        "  print(a,b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53R1fYWeUknu",
        "outputId": "27b4267d-24c8-4c57-dc52-f489926c9885"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch_geometric\n",
        "!pip install -q class_resolver\n",
        "!pip3 install pymatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TxXLqjPUm84"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import jaccard_score\n",
        "\n",
        "def iou(mask1, mask2):\n",
        "    x = mask1.ravel()\n",
        "    y = mask2.ravel()\n",
        "    intersection = np.logical_and(x, y)\n",
        "    union = np.logical_or(x, y)\n",
        "    similarity = np.sum(intersection)/ np.sum(union)\n",
        "    return similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEuiCblQUn7q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def cts(image, segmap,wnd = [20,20]):\n",
        "    \"\"\"\n",
        "    Crop the image and segmentation map to the boundary of the segmentation.\n",
        "\n",
        "    :param image: A numpy array representing the color image.\n",
        "    :param segmap: A numpy array representing the binary segmentation map.\n",
        "    :return: Cropped image and segmentation map.\n",
        "    \"\"\"\n",
        "    [ht,wdt] = segmap.shape\n",
        "    # Find the indices where segmap is 1\n",
        "    rows, cols = np.where(segmap == 255)\n",
        "    # Find min and max coordinates\n",
        "    min_row, max_row = max(min(rows)-wnd[0],0), min(max(rows)+wnd[0],ht)\n",
        "    min_col, max_col = max(min(cols)-wnd[1],0), min(max(cols)+wnd[1],wdt)\n",
        "\n",
        "    # Crop the image and segmap\n",
        "    cropped_image = image[min_row:max_row+1, min_col:max_col+1]\n",
        "    cropped_segmap = segmap[min_row:max_row+1, min_col:max_col+1]\n",
        "\n",
        "    return cropped_image, cropped_segmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqUEirElUoTj"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqyiHxjtUxZt"
      },
      "outputs": [],
      "source": [
        "%aimport segment\n",
        "%aimport util\n",
        "%aimport gnn_pool\n",
        "%aimport sim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lebOPmWYUxTx"
      },
      "outputs": [],
      "source": [
        "epochs = [55]\n",
        "\n",
        "# Number of clusters\n",
        "K = 2\n",
        "\n",
        "res = (224, 224)\n",
        "cut = 0\n",
        "mode = 0\n",
        "bs = True #False\n",
        "cc = False\n",
        "stride = 4\n",
        "facet = 'key'\n",
        "layer = 11\n",
        "pretrained_weights = './dino_deitsmall8_pretrain_full_checkpoint.pth'\n",
        "#download the pretrained_weights from https://dl.fbaipublicfiles.com/dino/dino_deitsmall8_pretrain/dino_deitsmall8_pretrain_full_checkpoint.pth\n",
        "in_dir = images\n",
        "out_dir = './results_CVCColon_GAT'\n",
        "log_bin = False\n",
        "############# for cut==0, alpha will act like threshold\n",
        "alpha = 0.5 #0.35 for ISIC2018\n",
        "######################################################\n",
        "save = True\n",
        "activ = \"SiLU_GAT\" #\"SiLU_GAT3\" for ISIC2018\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDqVx_GdLPc5"
      },
      "outputs": [],
      "source": [
        "from bilateral_solver import bilateral_solver_output\n",
        "from features_extract import deep_features\n",
        "from torch_geometric.data import Data\n",
        "from extractor import ViTExtractor\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import util\n",
        "import os\n",
        "from PIL import Image\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import asarray\n",
        "import tifffile as tiff\n",
        "from gnn_pool import GNNpool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-4Utlx_JxUq",
        "outputId": "e08a1a34-378a-4af3-8834-4cd40a995e65"
      },
      "outputs": [],
      "source": [
        "#this cell is only required for collective learning approach\n",
        "model_type = 'dino_vits8'\n",
        "feats_dim = 384\n",
        "extractor = ViTExtractor(model_type, 4, model_dir=pretrained_weights, device=device)\n",
        "model = GNNpool(feats_dim, 128, 32, K, device, activ).to(device)\n",
        "#uncomment the line below to load the model for passes of collective learning beyond the first\n",
        "#model.load_state_dict(torch.load('./modelCVC.pt', map_location=torch.device(device)))\n",
        "opt = optim.AdamW(model.parameters(), lr=0.0001)\n",
        "model.train()\n",
        "kkk = 0\n",
        "total_iou = 0\n",
        "imgcount = 1\n",
        "dir_name = 'CVC_CL_pass1'\n",
        "dirpath = os.path.join(out_dir,dir_name)\n",
        "for image, true_mask in tqdm(zip(images, masks)):\n",
        "    kkk += 1\n",
        "    filename = image.split('/')[-1].split('.')[0]\n",
        "    filetype = image.split('/')[-1].split('.')[-1]\n",
        "\n",
        "    if 'tif' in filetype:\n",
        "      from libtiff import TIFF\n",
        "      image = asarray(TIFF.open(image).read_image())\n",
        "    else:\n",
        "      image = asarray(Image.open(image))\n",
        "    true_mask = asarray(Image.open(true_mask).convert('L'))\n",
        "\n",
        "    image, true_mask = cts(image, true_mask)\n",
        "    true_mask = np.where(true_mask ==255, 1, 0).astype(np.uint8)\n",
        "\n",
        "    if 'tif' in filetype:\n",
        "      image_tensor, image = util.load_data_img1(image, res)\n",
        "    else:\n",
        "      image_tensor, image = util.load_data_img(image, res)\n",
        "    kkk += 1\n",
        "    # Extract deep features, from the transformer and create an adj matrix\n",
        "    F = deep_features(image_tensor, extractor, layer, facet, bin=log_bin, device=device)\n",
        "    W = util.create_adj(F, cut, alpha)\n",
        "        ###########################################################\n",
        "        # Data to pytorch_geometric format\n",
        "    node_feats, edge_index, edge_weight = util.load_data(W, F)\n",
        "    data = Data(node_feats, edge_index, edge_weight).to(device)\n",
        "        ##########################################################################################\n",
        "        # GNN pass\n",
        "        ##########################################################################################\n",
        "    for _ in range(5):\n",
        "        opt.zero_grad()\n",
        "        A, S = model(data, torch.from_numpy(W).to(device))\n",
        "        loss = model.loss(A, S)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        # polled matrix (after softmax, before argmax)\n",
        "    S = S.detach().cpu()\n",
        "    S = torch.argmax(S, dim=-1)\n",
        "        ##########################################################################################\n",
        "        # Post-processing Connected Component/bilateral solver\n",
        "        ##########################################################################################\n",
        "    mask0, S = util.graph_to_mask(S, cc, stride, image_tensor, image)\n",
        "        # apply bilateral solver\n",
        "    if bs:\n",
        "        #util.save_or_show([image, np.where(mask0==True, 255,0).astype(np.uint8), util.apply_seg_map(image, mask0, 0.5),true_mask*255], filename, dirpath1 ,save)\n",
        "        mask0 = bilateral_solver_output(image, mask0,sigma_spatial=11, sigma_luma=5, sigma_chroma=5)[1]\n",
        "    mask0 = np.where(mask0==True, 1,0).astype(np.uint8)\n",
        "\n",
        "    try:\n",
        "      cur_iou = iou(mask0, true_mask)\n",
        "      cur_iou1 = iou(1-mask0, true_mask)\n",
        "      if cur_iou < cur_iou1:\n",
        "        cur_iou = cur_iou1\n",
        "        mask0 = 1 - mask0\n",
        "      total_iou += cur_iou\n",
        "      print(total_iou/imgcount, imgcount,cur_iou)\n",
        "      imgcount  = imgcount +1\n",
        "    except:\n",
        "      pass\n",
        "    util.save_or_show([image, mask0*255, util.apply_seg_map(image, mask0, 0.5),true_mask*255], filename, dirpath ,save)\n",
        "torch.save(model.state_dict(), 'modelCVC.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2etYA1CxD9F",
        "outputId": "e2fda4fe-d028-4f82-a309-a952845096e6"
      },
      "outputs": [],
      "source": [
        "acc = segment.GNN_seg('./modelCVC.pt',mode, cut, alpha, epochs, K, pretrained_weights, images, masks, out_dir, save, cc, bs, log_bin, res, facet, layer, stride,device, iou,cts, activ)\n",
        "\n",
        "#modelName should be './model.pt' for non-CL runs"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
